{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bf7c4c9",
   "metadata": {},
   "source": [
    "# Using the COGITATE iEEG data \n",
    "Welcome to the COGITATE Consortium's iEEG Data Exploration Notebook! In this tutorial, we will guide you through the process of accessing and analyzing intracranial electroencephalography (iEEG) data collected as part of our consortium. The COGITATE consortium is an adversarial collaboration aiming to deepen our understanding of consciousness by comparing two prominent theories: Integrated Information Theory (IIT) and Global Neuronal Workspace (GNW).\n",
    "\n",
    "The dataset we are sharing originates from experiments employing a carefully designed paradigm. Participants were presented with images from four distinct categories (faces, objects, letters and symbols), each displayed at three different orientations (center, left, and right). Furthermore, these images were shown for varying durations of 0.5, 1.0, and 1.5 seconds. At the beginning of each experimental block, participants were shown two specific target identities, representing either a specific face and a specific object. Participants were tasked with pressing a button when these target identities appeared. This introduced an additional task relevance condition: stimuli of the same category as the targets but of a different identity were deemed task-relevant, while stimuli from other categories were considered task-irrelevant. This comprehensive experimental design allows us to delve into the neural correlates of consciousness across different stimuli and temporal conditions while controlling for task demands.\n",
    "\n",
    "This notebook serves a dual purpose: to provide a user-friendly guide on how to navigate, load, preprocess, and access specific data within our dataset, and to introduce the basics of iEEG data analysis. We will walk you through a simple univariate analysis, offering insights into individual channels, and a basic multivariate analysis, shedding light on the complex interplay of neural signals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0689f6e3-15a8-4161-b878-c803c6c3e8bb",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "By the end of this tutorial, you will:\n",
    "\n",
    "- Understand how to load and preprocess iEEG data from the COGITATE consortium\n",
    "- Access and index specific data based on experimental conditions\n",
    "- Perform a simple univariate analysis to examine the response patterns of individual channels\n",
    "- Engage in a basic multivariate analysis to uncover relationships and patterns across multiple channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aaf6ec-a18d-47a1-9b61-1143f8e0a810",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFCCCC; padding: 10px; border-radius: 5px;\">\n",
    "\n",
    "## Disclaimer:\n",
    "\n",
    "This notebook does not aim to replicate the analysis of the COGITATE consortium analyses ([SOURCE](insert_source_link_here)) but rather guide users to use our data. The code to replicate the results will be shared [here](insert_source_code_link_here).\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a44b57e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 files missing from root.txt in C:\\Users\\alexander.lepauvre\\mne_data\\MNE-sample-data\\subjects\n",
      "0 files missing from bem.txt in C:\\Users\\alexander.lepauvre\\mne_data\\MNE-sample-data\\subjects\\fsaverage\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\alexander.lepauvre\\\\mne_data\\\\MNE-sample-data\\\\subjects\\\\fsaverage'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting all modules:\n",
    "from pathlib import Path\n",
    "import scipy\n",
    "import mne\n",
    "from mne.viz import plot_alignment, snapshot_brain_montage\n",
    "from mne_bids import BIDSPath, read_raw_bids\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "from pingouin import ttest\n",
    "\n",
    "from pipelines.Preprocessing import preprocessing\n",
    "from pipelines.OnsetResponsiveness import onset_responsiveness\n",
    "from pipelines.Decoding import decoding\n",
    "from HelperFunctions import get_roi_channels, create_mni_montage, get_cmap_rgb_values\n",
    "\n",
    "import environment_variable as ev\n",
    "\n",
    "# Fetch the fsaverage brain surface to plot the electrodes localization across subjects\n",
    "fs_average_dir = Path(mne.datasets.sample.data_path(), 'subjects')\n",
    "# use mne-python's fsaverage data\n",
    "mne.datasets.fetch_fsaverage(subjects_dir=fs_average_dir, verbose=True)  # downloads if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ac0d63",
   "metadata": {},
   "source": [
    "## Set up and download of the BIDS converted data\n",
    "The first step is to specify the data that we will be working with, as well as the place to store them on the local machine. All path are specified under environment_variable.py. Make sure to edit this file to match your computer. For the purpose of this notebook, we will download the data of the first of the COGITATE for a couple of subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c1046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the path to the different data:\n",
    "print(ev.raw_root)\n",
    "print(ev.bids_root)\n",
    "print(ev.fs_directory)\n",
    "print(\"If this does not match what you expect, make sure to change the path under iEEG-data-release/environment_variable.py\")\n",
    "\n",
    "# Specify the subjects to work with:\n",
    "subjects_list = [\"SF102\", \"SF124\", \"SF125\"]\n",
    "# Further specify the data to use from the subjects:\n",
    "session = \"V1\"\n",
    "datatype = \"ieeg\"\n",
    "task = \"Dur\"\n",
    "\n",
    "# Create the mne bids path object, making access to the data easier. We will use one example subject to demonstrate the details of our analyses. \n",
    "bids_path = BIDSPath(root=ev.bids_rootbids_root, subject=subjects_list[0],\n",
    "                     session='V1',\n",
    "                     datatype='ieeg',\n",
    "                     task='Dur')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cb23f0",
   "metadata": {},
   "source": [
    "As the data are organized according to the BIDS specification, we can use the mne bids functionalities to load the data, which we will showcase with one test subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc0263d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_raw_bids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Loading the data under the term broadband, as it is what they are as long as no further\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# filtering was employed\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m raw \u001b[38;5;241m=\u001b[39m \u001b[43mread_raw_bids\u001b[49m(bids_path\u001b[38;5;241m=\u001b[39mbids_path, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m);\n",
      "\u001b[1;31mNameError\u001b[0m: name 'read_raw_bids' is not defined"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# Loading the data under the term broadband, as it is what they are as long as no further\n",
    "# filtering was employed\n",
    "raw = read_raw_bids(bids_path=bids_path, verbose=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03cab13-89e7-429b-84ff-f32e10504319",
   "metadata": {},
   "source": [
    "Now that the data are lodaded, we can inspect them by plotting  the power spectrum density as well as the electrodes localization on the brain surface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537e6c60-c0ef-4626-a348-5a4b9f7a4b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Compute and plot the PSD:\n",
    "raw.compute_psd().plot(picks='data', exclude='bads', spatial_colors=False)\n",
    "\n",
    "# Plot the channels localization on the subject's pial surface. \n",
    "fig = plot_alignment(\n",
    "    info=raw.info,\n",
    "    trans=\"fsaverage\",\n",
    "    subject=subjects_list[0],\n",
    "    subjects_dir=ev.fs_directory,\n",
    "    surfaces=[\"pial\"],\n",
    "    coord_frame=\"head\",\n",
    "    ecog=True, \n",
    "    seeg=False,\n",
    "    sensor_colors=sensor_colors,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f983877",
   "metadata": {},
   "source": [
    "## Running the preprocessing\n",
    "The BIDS downloaded data can then be preprocessed. In this code, we provide a preprocessing pipeline which applies the following\n",
    "steps:\n",
    "- Removal of bad channels \n",
    "- Notch filtering to remove line noise (60Hz)\n",
    "- Re-referencing using Laplace scheme\n",
    "- High gamma computations (70-150Hz)\n",
    "- ERP computations (0-30Hz)\n",
    "- Epoching\n",
    "(See here for more details)\n",
    "\n",
    "This preprocessing pipeline can be called using the provided function called preprocessing. This function requires 3 inputs:\n",
    "- config_file: a json file specifying all the details of the steps to perform\n",
    "- subjects_list: a list of the name of all the subjects on whom to apply the pipeline\n",
    "- bids_root: path to the bids folder on the local drive.\n",
    "\n",
    "The command below will apply the preprocessing pipeline as specified in the default config file of this repository, which follow\n",
    "the specification of this paper. The pipeline is highly configurable and can be adjusted by specifying different options,\n",
    "different order of steps and so on. If you wish to change the pipeline, we recommand you create a copy of the existing one\n",
    "and rename it according to your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b3110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "subjects_list = [\"SF102\"]\n",
    "config_file = \"pipelines/preprocessing_config-default.json\"\n",
    "preprocessing(config_file, subjects_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f0922e",
   "metadata": {},
   "source": [
    "### Visualizing the preprocessed data\n",
    "The final output of the preprocessing pipeline are the epoched data. The naming conventions of the folders follow the BIDS conventions\n",
    "You can therefore load the data as follows, and plot a single channel using mne functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c462a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "subject = 'SF102'\n",
    "channel = 'LO1'\n",
    "example_epochs_path = Path(ev.bids_root, 'derivatives', 'preprocessing',\n",
    "                           'sub-' + subject, 'ses-' + \"V1\", 'ieeg',\n",
    "                           \"epoching\", 'high_gamma',\n",
    "                           \"sub-{}_ses-{}_task-{}_desc-epoching_{}-epo.fif\".format(subject,\n",
    "                                                                                   \"V1\", \"Dur\",\n",
    "                                                                                   \"ieeg\"))\n",
    "\n",
    "epochs = mne.read_epochs(example_epochs_path, preload=True)\n",
    "mne.viz.plot_epochs_image(epochs, picks=channel, \n",
    "                          show=False, units=dict(ecog=\"HGP (norm.)\", seeg=\"HGP (norm.)\"),\n",
    "                          scalings=dict(ecog=1, seeg=1),\n",
    "                          evoked=True, cmap=\"RdYlBu_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7469f8",
   "metadata": {},
   "source": [
    "Notice that the figure above looks odd, with some activation in the baseline. That is because we have data that are epochs relative to both the onset and the offset of the stimuli for different analyses. For most purpose, we would need to select only the epochs locked to stimulus onset. This can be achieved by selecting the epochs by conditions, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4a6723",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "onset_locked_epochs = epochs['stimulus onset']\n",
    "mne.viz.plot_epochs_image(onset_locked_epochs, picks=channel, \n",
    "                          show=False, units=dict(ecog=\"HGP (norm.)\", seeg=\"HGP (norm.)\"),\n",
    "                          scalings=dict(ecog=1, seeg=1),\n",
    "                          evoked=True, cmap=\"RdYlBu_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c76ec",
   "metadata": {},
   "source": [
    "In general, if you wish to extract specific conditions, this data are equipped with forward slash separated strings, enabling you to select any condition by passing the according string or list of strings. Consult the documentation found XXX to find all the conditions\n",
    "that can be accessed from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ce19b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "task_relevant_epochs = epochs['stimulus onset/Relevant non-target']\n",
    "mne.viz.plot_epochs_image(task_relevant_epochs, picks=channel, \n",
    "                          show=False, units=dict(ecog=\"HGP (norm.)\", seeg=\"HGP (norm.)\"),\n",
    "                          scalings=dict(ecog=1, seeg=1),\n",
    "                          evoked=True, cmap=\"RdYlBu_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d6cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "task_relevant_face_short_epochs = epochs['stimulus onset/Relevant non-target/face/500ms']\n",
    "mne.viz.plot_epochs_image(task_relevant_face_short_epochs, picks=channel, \n",
    "                          show=False, units=dict(ecog=\"HGP (norm.)\", seeg=\"HGP (norm.)\"),\n",
    "                          scalings=dict(ecog=1, seeg=1),\n",
    "                          evoked=True, cmap=\"RdYlBu_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaa3853",
   "metadata": {},
   "source": [
    "## Onset responsiveness\n",
    "Now that the data were preprocessed, we can apply the onset responsiveness analysis, which consists in comparing single channel activation before and after visual stimuli onsets. This enables determining which are the channels which are responsive to our task in general. To do so, we will compute a Bayes factor using the JZS Bayes Factor approximation described in [SOURCE]. This method consists in computing a paired t-test and using a formula to generate a Bayes factor. The main advantage of this approach is that it alleviates the need for multiple comparison correction. This way, the results will not change as a function of the number of subjects that gets added. \n",
    "\n",
    "Practically, we will first extract the trials locked to the onset of the stimuli and remove the target trials (as these require participants responses). We will then extract the pre (-0.3 to 0 sec. from stimulus onset) and post-stimulus (0.05 to 0.35 sec.) presentation segments for each trial. The data within each segment will be aggregated by computing the area under the curve (np.trapz). Finally, we will use the pingouin function ttest with the Cauchy scale factor set to 0.707 (default value in pingouin). \n",
    "\n",
    "We will first show an example on a single channel then proceed to show how to run it on all electrodes of selected subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bc6d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the trials of interest:\n",
    "onset_responsiveness_data =  epochs[[\"stimulus onset/Irrelevant\", \"stimulus onset/Relevant non-target\"]]\n",
    "# Crop the data:\n",
    "prestim_data = np.squeeze(onset_responsiveness_data.copy().crop(tmin=-0.3, tmax=0).get_data(picks='LO1'))\n",
    "poststim_data = np.squeeze(onset_responsiveness_data.copy().crop(tmin=0.05, tmax=0.350).get_data(picks='LO1'))\n",
    "# Compute the AUC:\n",
    "prestim_auc = np.trapz(prestim_data, axis=1)\n",
    "poststim_auc = np.trapz(poststim_data, axis=1)\n",
    "# Perform a two sided paired t-test:\n",
    "results = ttest(prestim_auc, poststim_auc, paired=True, r=0.707, alternative='two-sided').round(3)\n",
    "\n",
    "# Showing the results of the test:\n",
    "print('T-test results:')\n",
    "print('   T-stat={}'.format(results['T'].item()))\n",
    "print('   p-value={}'.format(results['p-val'].item()))\n",
    "print('   BF={}'.format(results['BF10'].item()))\n",
    "print('   df={}'.format(results['dof'].item()))\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(poststim_auc - prestim_auc)\n",
    "ax.set_title('Poststim - Prestim')\n",
    "ax.set_xticks([])\n",
    "ax.set_xticklabels([])\n",
    "ax.set_ylabel('AUC difference')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22990923-fff8-48a6-ae36-c8b254efed64",
   "metadata": {},
   "source": [
    "As we can see, the difference between pre and post-stimulus window is positive, indicating that HG activation is larger after stimulus onset. Accordingly, the bayes factor is really large, indicating that we have very strong evidence that this channel is responsive to our experimental paradigm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f693e297",
   "metadata": {},
   "source": [
    "### Onset responsiveness pipeline\n",
    "The above example was applied to a single channel from a single participant. We further provide pipelines that enable the \n",
    "computation of onset responsiveness on all channels and subjects. As in the case of the preprocessing pipeline, this can\n",
    "be achieved by calling the function onset_responsiveness which takes 3 mandatory inputs:\n",
    "- config_file: a json file specifying the details of the analysis, such as the trials on which to apply the pipeline, pre and post stimuli time windows, metric to use...\n",
    "- subjects_list: a list of the name of all the subjects on whom to apply the pipeline\n",
    "- bids_root: path to the bids folder on the local drive.\n",
    "Just as in the case of the pre-processing, the config files are highly configurable with many additional options. Make sure to visit XXX to investigate these options and how to specify them.\n",
    "\n",
    "This function further takes two additional inputs:\n",
    "- plot_single_channels: boolean flag to plot single electrodes or not\n",
    "- plot_only_responsive: boolean flag to plot only those channels that were found to have differences in activation compared to baseline. \n",
    "\n",
    "Single channels plotting can take a long time, so you might want to deactivate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5936c0c4-3081-473c-8fd7-a6b7b967c4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "subjects_list = [\"SF102\"]\n",
    "config_file = \"pipelines/onset_responsiveness_config-default.json\"\n",
    "results = onset_responsiveness(config_file, subjects_list, plot_single_channels=False, plot_only_responsive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8268a151",
   "metadata": {},
   "source": [
    "### Investigate the results\n",
    "The pipeline generates a dataframe containing the results of the onset responsiveness test, with one row per channel, with the results of the test in the columns:\n",
    "Now that we have ran the onset responsiveness analysis, we can provide some summary statistics for the \n",
    "data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f28d9d-30ff-435d-b5c1-e559992d95af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd91b84-dd70-4639-b3b9-d3429350e084",
   "metadata": {},
   "source": [
    "We can investigate the results to assess how many electrodes show a change in activation, and for how many activation increases and decreases compared to the baseline period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605841f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channels counts:\n",
    "responsive_channels = results[results['reject'] == True]\n",
    "activated_channels = results[(results['reject'] == True) & (results['f_size'] > 0)]\n",
    "deactivated_channels = results[(results['reject'] == True) & (results['f_size'] < 0)]\n",
    "# Print the counts:\n",
    "print(\"{} out of {} ({:.2f}%) were found to be onset responsive!\".format(\n",
    "    responsive_channels.shape[0], \n",
    "    results.shape[0],\n",
    "    responsive_channels.shape[0] / results.shape[0] * 100))\n",
    "print(\"{} out of {} ({:.2f}%) showed an increase of activation following  stimulus onset!\".format(\n",
    "    activated_channels.shape[0], \n",
    "    results.shape[0],\n",
    "    activated_channels.shape[0] / results.shape[0] * 100\n",
    "))\n",
    "print(\"{} out of {} ({:.2f}%) showed an decrease of activation following  stimulus onset!\".format(\n",
    "    deactivated_channels.shape[0], \n",
    "    results.shape[0],\n",
    "    deactivated_channels.shape[0] / results.shape[0] * 100\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7650e2",
   "metadata": {},
   "source": [
    "We can also further investigate the results by looking at each subject separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9369409",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_channels_counts = results.groupby([\"subject\", \"reject\"]).count().reset_index()[[\"subject\", \"reject\", \"channel\"]]\n",
    "# Print the counts:\n",
    "print(subjects_channels_counts.loc[subjects_channels_counts[\"reject\"] == True, [\"subject\", \"channel\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51381a48",
   "metadata": {},
   "source": [
    "### Plotting responsive channels on the brain surface\n",
    "One important sanity check is to make sure that the localization of the effects make sense. As \n",
    "the task in the COGITATE was visual, one would expect to find a lot of onset responsive electrodes\n",
    "over the posterior cortex. To check whether that is the case, we will extract only the onset responsive\n",
    "electrodes that we hve identified above and we will plot that on the pial surface. \n",
    "\n",
    "This step require MNE setup to have been performed as to allow 3D rendering. Consult the MNE webpage: XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16920788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the onset responsive electrodes:\n",
    "onset_responsive_results = results.loc[results[\"reject\"] == True]\n",
    "# Get these channels localization in MNI space:\n",
    "mni_info = create_mni_montage(onset_responsive_results[\"channel\"].to_list(), bids_path, ev.fs_directory, fs_average_dir)\n",
    "\n",
    "# We can now go ahead and plot the ecog channels on the brain:\n",
    "# Plot the ecog:\n",
    "fig = plot_alignment(\n",
    "    info=mni_info,\n",
    "    trans=\"fsaverage\",\n",
    "    subject=\"fsaverage\",\n",
    "    subjects_dir=fs_average_dir,\n",
    "    surfaces=[\"pial\"],\n",
    "    coord_frame=\"head\",\n",
    "    ecog=True, \n",
    "    seeg=False,\n",
    "    sensor_colors=[[1.0, 1.0, 1.0, 0.8]] * len(mni_info.ch_names),\n",
    ")\n",
    "mne.viz.set_3d_view(fig, azimuth=180, elevation=70, focalpoint=\"auto\", distance=0.4)\n",
    "xy, im = snapshot_brain_montage(fig, mni_info)\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.imshow(im_ecog)\n",
    "ax2.yaxis.set_visible(False)\n",
    "ax2.xaxis.set_visible(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9e871b",
   "metadata": {},
   "source": [
    "Additionally, not only do we expect more channels to be responsive in posterior regions, we also expect the effects to be stronger. To vizualize this, you can plot the effect sizes of the onset\n",
    "responsiveness on the brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff84e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the effect sizes:\n",
    "f_sizes = onset_responsive_results[\"f_size\"].to_list()\n",
    "\n",
    "# Get the RGB values for effect sizes mapped onto a colormap\n",
    "sensor_colors = get_cmap_rgb_values(f_sizes, cmap=None, center=0)\n",
    "\n",
    "# We can now go ahead and plot the dig montage on the brain:\n",
    "fig = plot_alignment(\n",
    "    info=mni_info,\n",
    "    trans=\"fsaverage\",\n",
    "    subject=\"fsaverage\",\n",
    "    subjects_dir=fs_average_dir,\n",
    "    surfaces=[\"pial\"],\n",
    "    coord_frame=\"head\",\n",
    "    ecog=True, \n",
    "    seeg=False,\n",
    "    sensor_colors=sensor_colors,\n",
    ")\n",
    "mne.viz.set_3d_view(fig, azimuth=180, elevation=70, focalpoint=\"auto\", distance=0.4)\n",
    "\n",
    "xy, im = snapshot_brain_montage(fig, mni_info)\n",
    "xy_pts = np.vstack([xy[ch] for ch in mni_info.ch_names])\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.imshow(im)\n",
    "ax2.yaxis.set_visible(False)\n",
    "ax2.xaxis.set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ae28c7-329b-4f8e-9545-0d8287de8c97",
   "metadata": {},
   "source": [
    "The colorbar is centered on 0, such that channels showing an increase in effect size are plotted in yellow/red tones while the channels showing a decrease in activation are plotted in blueish tones. As is to be expected, the channels located in occipital regions show the strongest effect sizes, but onset responsive electrodes can be found in several cortical locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6420fa34",
   "metadata": {},
   "source": [
    "### Investigate specific regions of interest\n",
    "We further provide functionalities to extract channels located in specific regions of interest. This will be illustrated by extracting channels located within the visual cortex. We will extract all channels located within the G_cuneus, G_occipital_sup, G_occipital_sup, Pole_occipital, S_calcarine, S_oc_sup_and_transversal, S_oc_middle_and_Lunatus as defined by the Destrieux atlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26776f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of regions of interest:\n",
    "rois = [\"Pole_occipital\",\n",
    "        \"G_occipital_middle\",\n",
    "        \"G_cuneus\",\n",
    "       ]\n",
    "# Extract the channels within this ROI:\n",
    "roi_channels = get_roi_channels(epochs.ch_names, rois, bids_path, 'aparc.a2009s+aseg')\n",
    "print('The following channels were found in the ROI')\n",
    "print(roi_channels)\n",
    "# Extract the onset responsiveness results for the channels only:\n",
    "roi_onset_responsiveness = results.loc[results['channel'].isin(['-'.join(['SF102', channel]) for channel in roi_channels])]\n",
    "channels_counts = roi_onset_responsiveness.groupby(\"reject\").count().reset_index()[[\"reject\", \"channel\"]]\n",
    "\n",
    "# Create a montage for these channels alone:\n",
    "roi_mni_info = create_mni_montage(roi_onset_responsiveness[\"channel\"].to_list(), bids_path, ev.fs_directory, fs_average_dir)\n",
    "\n",
    "# Extract the effect sizes:\n",
    "roi_f_sizes = roi_onset_responsiveness[\"f_size\"].to_list()\n",
    "\n",
    "# Get the RGB values for effect sizes mapped onto a colormap\n",
    "roi_sensor_colors = get_cmap_rgb_values(roi_f_sizes, cmap=\"Oranges\", center=None)\n",
    "\n",
    "# We can now go ahead and plot the dig montage on the brain:\n",
    "fig = plot_alignment(\n",
    "    info=roi_mni_info,\n",
    "    trans=\"fsaverage\",\n",
    "    subject=\"fsaverage\",\n",
    "    subjects_dir=fs_average_dir,\n",
    "    surfaces=[\"pial\"],\n",
    "    coord_frame=\"head\",\n",
    "    ecog=True, \n",
    "    seeg=False,\n",
    "    sensor_colors=roi_sensor_colors,\n",
    ")\n",
    "mne.viz.set_3d_view(fig, azimuth=180, elevation=70, focalpoint=\"auto\", distance=0.4)\n",
    "\n",
    "xy, im = snapshot_brain_montage(fig, mni_info)\n",
    "xy_pts = np.vstack([xy[ch] for ch in mni_info.ch_names])\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.imshow(im)\n",
    "ax2.yaxis.set_visible(False)\n",
    "ax2.xaxis.set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16647c95",
   "metadata": {},
   "source": [
    "## Decoding\n",
    "In addition to investigate the univariate responses of every single channel, we can also performed a multivariate analysis, investigating the presence of representation of particular contrasts of interests across the electrodes in our sample. In the current example, we will\n",
    "use support vector machine to decode faces vs. objects across all electrodes within our example subject.\n",
    "\n",
    "We will perform temporal generalization decoding using support vector machine for the faces vs objects labels in the task irrelevant trials. We will first prepare the data by selecting only seeg and ecog channels, downsample the data and select the conditions of interest. We will then create a classifier and apply it to our data with 5 fold cross validation and score the decoding accuracy using roc_auc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80610c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries:\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from mne.decoding import (GeneralizingEstimator, cross_val_multiscore)\n",
    "\n",
    "\n",
    "#Select electrodes:\n",
    "picks = mne.pick_types(epochs.info, ecog=True, seeg=True)\n",
    "decoding_epochs = epochs.copy().pick(picks)\n",
    "# Downsample:\n",
    "decoding_epochs.resample(30, n_jobs=1)\n",
    "# Select trials:\n",
    "decoding_epochs = decoding_epochs[[\"stimulus onset/Irrelevant/face\", \"stimulus onset/Irrelevant/object\"]]\n",
    "\n",
    "# Create the classifier:\n",
    "clf_steps = []\n",
    "clf_steps.append(StandardScaler())\n",
    "clf_steps.append(svm.SVC(kernel='linear', class_weight='balanced'))\n",
    "clf = make_pipeline(*clf_steps)\n",
    "# Temporal generalization estimator:\n",
    "time_gen = GeneralizingEstimator(clf, n_jobs=1, scoring='roc_auc',\n",
    "                                 verbose=\"ERROR\")\n",
    "\n",
    "# Extract the data:\n",
    "data = decoding_epochs.get_data()\n",
    "# Get the classes:\n",
    "y = decoding_epochs.metadata[\"category\"].values\n",
    "# Run the decoding:\n",
    "scores = cross_val_multiscore(time_gen, data, y, cv=5, n_jobs=None)\n",
    "\n",
    "# Plot the results:\n",
    "fig, ax = plt.subplots()\n",
    "# Plot matrix with transparency:\n",
    "im = ax.imshow(np.mean(scores, axis=0), cmap=\"RdYlBu_r\",\n",
    "               extent=[epochs.times[0], epochs.times[-1], epochs.times[0], epochs.times[-1]],\n",
    "               origin=\"lower\",  # aspect=\"equal\",\n",
    "               interpolation=\"lanczos\", vmin=0, vmax=1)\n",
    "# Add the axis labels and so on:\n",
    "ax.set_xlim([epochs.times[0], epochs.times[-1]])\n",
    "ax.set_ylim([epochs.times[0], epochs.times[-1]])\n",
    "ax.set_xlabel(\"Testing time (s)\")\n",
    "ax.set_ylabel(\"Training time (s)\")\n",
    "ax.set_title(\"Decoding of {}\".format(\"faces vs. objects\"))\n",
    "ax.axvline(0, color='k')\n",
    "ax.axhline(0, color='k')\n",
    "fig.colorbar(im, fraction=0.046, pad=0.04)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb3d599",
   "metadata": {},
   "source": [
    "### Decoding pipeline\n",
    "The above example was applied to a single participant. We further provide pipelines to perform decoding on all subjects. This can be achieved by calling the function decoding which takes 3 mandatory inputs:\n",
    "\n",
    "- config_file: a json file specifying the details of the analysis, such as trials to select, decoding targets, cross task generalization...\n",
    "- subjects_list: a list of the name of all the subjects on whom to apply the pipeline\n",
    "- bids_root: path to the bids folder on the local drive. Just as in the case of the pre-processing, the config files are highly configurable with many additional options. Make sure to visit XXX to investigate these options and how to specify them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7391ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_list = [\"SF102\"]\n",
    "config_file = \"pipelines/decoding_config-default.json\"\n",
    "scores = decoding(config_file, subjects_list, bids_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26350e51",
   "metadata": {},
   "source": [
    "### Decoding within regions of interest\n",
    "In the case of decoding, it is especially relevant to be able to investigate spatial specificy. This can be achieved as shown in the onset responsiveness analysis by pre-selecting channels based on their anatomical labels. We will here show how to do so by selecting channels located within a posterior ROI defined by the following labels:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dde48cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rois = [\"Pole_occipital\",\n",
    "        \"G_occipital_middle\",\n",
    "        \"G_cuneus\",]\n",
    "# Extract the channels within this ROI:\n",
    "roi_channels = get_roi_channels(epochs.ch_names, rois, bids_path, 'aparc.a2009s+aseg')\n",
    "print('The following channels were found in the ROI')\n",
    "print(roi_channels)\n",
    "\n",
    "# Selecting those channels only:\n",
    "roi_epochs = epochs.copy().pick(roi_channels)\n",
    "# Downsample:\n",
    "roi_epochs.resample(30, n_jobs=1)\n",
    "# Select trials:\n",
    "roi_epochs = roi_epochs[[\"stimulus onset/Irrelevant/face\", \"stimulus onset/Irrelevant/object\"]]\n",
    "\n",
    "# Create the classifier:\n",
    "clf_steps = []\n",
    "clf_steps.append(StandardScaler())\n",
    "clf_steps.append(svm.SVC(kernel='linear', class_weight='balanced'))\n",
    "clf = make_pipeline(*clf_steps)\n",
    "# Temporal generalization estimator:\n",
    "time_gen = GeneralizingEstimator(clf, n_jobs=1, scoring='roc_auc',\n",
    "                                 verbose=\"ERROR\")\n",
    "\n",
    "# Extract the data:\n",
    "data = roi_epochs.get_data()\n",
    "# Get the classes:\n",
    "y = roi_epochs.metadata[\"category\"].values\n",
    "# Run the decoding:\n",
    "scores = cross_val_multiscore(time_gen, data, y, cv=5, n_jobs=None)\n",
    "\n",
    "# Plot the results:\n",
    "fig, ax = plt.subplots()\n",
    "# Plot matrix with transparency:\n",
    "im = ax.imshow(np.mean(scores, axis=0), cmap=\"RdYlBu_r\",\n",
    "               extent=[epochs.times[0], epochs.times[-1], epochs.times[0], epochs.times[-1]],\n",
    "               origin=\"lower\",  # aspect=\"equal\",\n",
    "               interpolation=\"lanczos\", vmin=0, vmax=1)\n",
    "# Add the axis labels and so on:\n",
    "ax.set_xlim([epochs.times[0], epochs.times[-1]])\n",
    "ax.set_ylim([epochs.times[0], epochs.times[-1]])\n",
    "ax.set_xlabel(\"Testing time (s)\")\n",
    "ax.set_ylabel(\"Training time (s)\")\n",
    "ax.set_title(\"Decoding of {}\".format(\"faces vs. objects\"))\n",
    "ax.axvline(0, color='k')\n",
    "ax.axhline(0, color='k')\n",
    "fig.colorbar(im, fraction=0.046, pad=0.04)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "mne"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
